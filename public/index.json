[{"body":"","link":"http://localhost:1313/","section":"","tags":null,"title":""},{"body":"","link":"http://localhost:1313/tags/docker/","section":"tags","tags":null,"title":"Docker"},{"body":"","link":"http://localhost:1313/tags/homelab/","section":"tags","tags":null,"title":"Homelab"},{"body":"","link":"http://localhost:1313/tags/netbox/","section":"tags","tags":null,"title":"Netbox"},{"body":"","link":"http://localhost:1313/post/","section":"post","tags":[""],"title":"Posts"},{"body":"","link":"http://localhost:1313/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"http://localhost:1313/tags/terraform/","section":"tags","tags":null,"title":"Terraform"},{"body":"Organizing my homelab with NetBox \u0026amp; Terraform Another Sunday, another day to dive into the homelab!\nWhile I'm waiting for an additional component to arrive (stay tuned for that!), I figured it's a good time to get a bit more organized. Over time, it‚Äôs become harder to track my devices, \u0026quot;racks\u0026quot; (okay, to be fair, I don‚Äôt really need racks yet ‚Äì see this old blog post), and most importantly, all of my IP addresses and VLANs.\nI‚Äôve been wanting to explore NetBox for a while now, and this felt like the right moment to give it a proper test run.\nüöÄ Getting Started with NetBox I'm running NetBox via Docker on a VM hosted on my Proxmox server.\nTo set it up, I used the official NetBox Docker guide:\nüëâ https://github.com/netbox-community/netbox-docker\nThe setup process went smooth and straightforward.\nSo lets dive into Netbox\nBut hey ‚Äì clicking through a UI and manually setting things up? That‚Äôs not 2025. This is why I decided to manage NetBox with Terraform.\nBonus: I also added my Windows DNS and private AdGuard server to Terraform, so I can keep track of all my used IP addresses in one centralized, without adding DNS records manually.\nüß∞ Terraform Setup Overview (I won‚Äôt be covering basic Terraform usage ‚Äì there are plenty of great tutorials out there.)\nüîß File Structure To keep things clean, I split my Terraform code into logical chunks:\nprovider.tf: Defines providers and authentication dcim.tf: For Data Center Infrastructure Management ipam.tf: For IP Address Management dns.tf: For DNS Management These files describe how and what resources terraform should create\nAdditionally, I‚Äôve got:\nvariables.tf: Declares all input variables terraform.tfvars: Stores my specific input values I‚Äôve tried to keep things as simple as possible.\nThis is my current provider configuration\nprovider.tf\n1terraform { 2 required_providers { 3 netbox = { 4 source = \u0026#34;e-breuninger/netbox\u0026#34; 5 version = \u0026#34;4.1.0\u0026#34; 6 } 7 dns = { 8 source = \u0026#34;hashicorp/dns\u0026#34; 9 version = \u0026#34;~\u0026gt; 3.0\u0026#34; 10 } 11 adguard = { 12 source = \u0026#34;gmichels/adguard\u0026#34; 13 version = \u0026#34;1.6.2\u0026#34; 14 } 15 } 16} 17 18# example provider configuration for https://demo.netbox.dev 19provider \u0026#34;netbox\u0026#34; { 20 server_url = \u0026#34;http://192.168.169.2:8000\u0026#34; 21 api_token = \u0026#34;insertapitokenhere\u0026#34; 22 skip_version_check = true 23} 24 25# Configure the DNS Provider 26provider \u0026#34;dns\u0026#34; { 27 update { 28 server = \u0026#34;10.20.10.10\u0026#34; 29 } 30} 31 32provider \u0026#34;adguard\u0026#34; { 33 host = \u0026#34;192.168.169.53:80\u0026#34; 34 username = \u0026#34;admin\u0026#34; 35 password = var.adguard_password 36 scheme = \u0026#34;http\u0026#34; # defaults to https 37 timeout = 5 # in seconds, defaults to 10 38 insecure = true # when `true` will skip TLS validation 39} üóÇÔ∏è Organizing Infrastructure with Variables I split my input variables (terraform.tfvars) into three main sections:\nPhysical Infrastructure red\nRacks, sites, and devices Home Network orange\nNetwork configuration and device mappings Lab Environment green\nVLANs, test devices, lab-specific segments These input variables map directly to Terraform resources.\nüîÑ Example: Managing VLANs Here‚Äôs a quick look at how I manage VLANs using Terraform. I will not cover every section of my terraform, you can look at the full code at my github https://github.com/p3t35/homelab-terraform\nEach VLAN is defined in a variable as part of an array. Every item in that array contains the full dataset needed for NetBox.\nSo if I want to add a new VLAN? I simply copy a row in the array and tweak the values. Terraform takes care of the rest. üôå\nAnd here‚Äôs how that variable gets mapped to a resource:\nAs you can see, the information from each dataset is dynamically pulled into different resources. No need to touch each resource block when I want to create or update VLANs. This makes it easy and smooth to add additional information. With the help of for_each loops in terraform you can handle this datasets very clean.\nThis is what the result from the terraform code looks like: Whit this method I also managed my Windows DNS entries and my home adguard dns rewrites. This is also a good example of using different provider and resource types for creating dns entries from one variable file.\nI hope this gives a clearer idea of how I‚Äôm using NetBox and Terraform to automate and organize my homelab. There‚Äôs still a lot to do, but this is a solid foundation to build on.\nIn the end, I built a Terraform-based setup that might be a bit overengineered ‚Äî but it keeps everything organized and in one place. I'm sure this codebase will continue to evolve over time, with new providers and systems added along the way.\nThis should give you a quick introduction and idea how to manage stuff with terraform, depending on your needs and environment this could be expanded with other resources. For example you could managed your firewall rules or virtual machines with terraform. For nearly everything there is an terraform provider out there - so give it a try ;)\nIn more andanved setups you can combine terraform with CI/CD workflows to automate this hole process further. May I will cover this in another blog post. So stay tuned! üòä\nLast but not least, a quick shoutout to Visual Studio Code and its GitHub Copilot feature. It‚Äôs made writing code so much easier and was a huge help throughout this project.\nHere are three examples of how I used Copilot to write Terraform code:\nIn the first example I needed to add prefixes to each VLAN, so I explained my need very quick. In the second example I wanted to combine two variables which went pretty easy More updates soon!\n","link":"http://localhost:1313/post/netbox-terraform/","section":"post","tags":["homelab","terraform","netbox","docker"],"title":"Terraform and Netbox - meets homelab"},{"body":"","link":"http://localhost:1313/tags/esxi/","section":"tags","tags":null,"title":"Esxi"},{"body":"","link":"http://localhost:1313/tags/intelnuc/","section":"tags","tags":null,"title":"Intelnuc"},{"body":"","link":"http://localhost:1313/tags/vcf/","section":"tags","tags":null,"title":"Vcf"},{"body":"","link":"http://localhost:1313/tags/vcf9/","section":"tags","tags":null,"title":"Vcf9"},{"body":"VCF 9 Is Around the Corner ‚Äì Time to Upgrade the Homelab With VCF 9 right around the corner, it‚Äôs the perfect moment to give the homelab a long overdue upgrade.\nNew Hardware: 8-Port 2.5Gbit Switch To kick things off, I picked up an 8-port 2.5 Gbit switch from AliExpress for just ‚Ç¨54. Surprisingly, it performs well for its price and fits nicely into the homelab setup - the webui is a little \u0026quot;oldschool\u0026quot; but those its job. It‚Äôs a great value option for those looking to upgrade their networking without spending a fortune.\nGoodbye Nested Hosts and hello Traditional NFS Thanks to the reduced requirements and improved flexibility of VCF 9, I‚Äôve decided to deploy VCF 9 without nested ESXi hosts.\nI‚Äôm currently missing a third NUC or a dedicated storage host, so I opted for a improvised storage solution for providing NFS storage to my lab.\nProxmox + USB NIC + Ubuntu VM = DIY NFS I had a 2.5 Gbit USB 3.0 adapter lying around, which I connected to my always-on Proxmox node. On this node, an Ubuntu VM runs with a single NVMe drive acting as the shared storage.\nAt first, I wasn‚Äôt sure if this setup would even work‚Äîbut hey, why not give it a try?\nAdditionally, I migrated my pfSense firewall to this Proxmox node as well, isolating firewall responsibilities from my lab hosts.\nAnd this is what the current state looks like: ESXi with NVMe Tiering For the ESXi nodes, I went with NVMe tiering, which is now a fully supported feature in ESXi 9. Setup was straightforward, and everything installed smoothly.\nVCF 9 Installation Experience The new VCF 9 installation UI is a major improvement. It‚Äôs much easier and more flexible, especially for ‚Äúspecial‚Äù configurations that previously required manually tweaking JSON files (looking at you, VCF 5.2).\nDeployment went better than expected:\nEdge node deployment worked on my limited setup Storage latency is not ideal but works The Windows jump host feels a bit laggy ‚Ä¶but for testing and getting hands-on experience with VCF 9, it‚Äôs surprisingly good.\nLooking Ahead There‚Äôs definitely room for future improvements, particularly in the storage department, but I have to say‚ÄîVCF 9 is a big leap forward for the VCF platform overall.\nFinal Thoughts That‚Äôs the current state of my lab. I mainly wanted to showcase what‚Äôs possible with limited hardware and encourage others to tinker, test, and see what you can achieve with some creativity and the right tools.\nUntil next time‚Äîkeep homelabbing!\nLast but not least, a real-world picture of what an improvised homelab looks like: \u0026quot;the more you look, the worse it gets\u0026quot;.\n","link":"http://localhost:1313/post/vcf9-2-node-setup/","section":"post","tags":["homelab","intelnuc","vcf","vsphere","esxi","vcf9"],"title":"VCF9 2 Node setup"},{"body":"","link":"http://localhost:1313/tags/vsphere/","section":"tags","tags":null,"title":"Vsphere"},{"body":"Deploying VMware Cloud Foundation (VCF) on a Small Homelab ‚Äì Is It Possible? Looking at the official requirements for VMware Cloud Foundation (VCF), they seem excessive for a small homelab setup. But is it still possible? Let's give it a try‚Äîthe automated way!\nAutomating the VCF Deployment Deploying VCF manually is a time-consuming task. Preparing virtual ESXi hosts, configuring networking, and setting up storage can take hours. I wanted a more efficient solution‚Äîa script that could deploy VCF with just one click.\nThat's when I came across William Lam‚Äôs blog post about VCF deployment. His PowerShell script automates most of the setup, making the process much easier. However, there was one major limitation:\nThe script requires a shared datastore between the ESXi hosts, typically a vSAN datastore. Since I only have two Intel NUCs, vSAN wasn't an option due to the missing third witness node. I also don‚Äôt have the necessary storage for an NFS or iSCSI share. Adapting the Script for My Setup To make this work, I had to modify the script to fit my environment. The first challenge was that the script requires several inputs, which wasn‚Äôt an issue‚Äîuntil it came to selecting a datastore. The script only allows specifying one datastore, which didn't work for my setup.\nSo lets see what changes needs to be done to get the script to work on our limited setup.\nChanges I Made To successfully deploy VCF on my two-node cluster, I modified the script in several ways:\nReduced the NSX Manager size to small to save resources. 1$NSXManagerSize = \u0026#34;small\u0026#34; Lowered the nested ESXi CPU count from 12 to 8 to fit within my available hardware. 1$NestedESXiMGMTvCPU = \u0026#34;8\u0026#34; Modified the datastore variable to match my NUC-specific naming pattern, allowing me to use individual NVMe disks. 1$VMDatastorePattern = \u0026#34;*NVMe*\u0026#34; #pattern for local vmfs *NVMe* Removed workload domain input, as deploying additional workload domains is impractical for my small homelab. Adjusted the deployment logic for nested ESXi hosts: I distributed the four nested ESXi hosts across my two NUCs. Odd-numbered hosts go on esxi01, while even-numbered hosts go on esxi02. This ensures an even distribution of nested ESXi hosts across the two-node cluster. 1$counter = 0 #counter to track the deployments 2 $NestedESXiHostnameToIPsForManagementDomain.GetEnumerator() | Sort-Object -Property Value | Foreach-Object { 3 $VMName = $_.Key 4 $VMIPAddress = $_.Value 5 6 $evenorodd = $counter % 2 #check if counter is odd or even 7 $vmhost = ($cluster | Get-VMhost | Sort-Object)[$evenorodd] #select esxi host 8 $datastore = ($vmhost | Get-Datastore -Name $VMDatastorePattern) #select datastore with the pattern Ensure Cloudbuilder is also deployed on a local Datastore 1if($deployCloudBuilder -eq 1) { 2 $vmhost = ($cluster | Get-VMhost | Sort-Object)[0] 3 $datastore = ($vmhost | Get-Datastore -Name $VMDatastorePattern) Shutdown vCenter in order to save some extra resources ;) 1My-Logger \u0026#34;Going to wait 240 seconds then turning off vcenter to safe resources...\u0026#34; 2sleep 240 3 4Connect-VIServer -server $VIServer -user $VIUsername -password $VIPassword 5Get-VM -Name vcsa | Stop-VMGuest -Confirm:$false 6 7Disconnect-VIServer * -Confirm:$false For instruction how to run the script and adjust it you can have a look at the original github repo from william lam. https://github.com/lamw/vcf-automated-lab-deployment\nAs we can see both NUCs are heavy loaded with the nested ESXi Hosts, but all good to get a overview and first touchpoints with VCF.\nCleanup Script for Easy Reset To make experimenting with VCF deployments easier, I also created a cleanup script. This script allows me to completely remove the VCF installation, clean up the environment, and restart vCenter‚Äîready for a fresh deployment.\nThe cleanup script performs the following tasks:\nConnects to the Intel NUCs. Stops and removes all nested ESXi hosts. Start the powered off vCenter. Removes any leftover vApp from the vCenter inventory. 1$vcentervm = \u0026#34;vcsa\u0026#34; 2$vcenterip = \u0026#34;10.10.5.10\u0026#34; 3$vcenterpw = \u0026#34;VMware1!\u0026#34; 4$esxis = @(\u0026#34;10.10.5.250\u0026#34;, \u0026#34;10.10.5.251\u0026#34;) 5$esxipassword = \u0026#39;VMware1!\u0026#39; 6 7foreach ($esxi in $esxis) { 8 Connect-VIServer -server $esxi -user root -password $esxipassword 9 Get-VM -Name vcf-* | Stop-VM -Confirm:$false -ErrorAction SilentlyContinue 10 Get-VM -Name vcf-* | Remove-VM -DeletePermanently -Confirm:$false 11 if (Get-VM -Name $vcentervm -ErrorAction SilentlyContinue) { 12 Get-VM -Name $vcentervm | Start-VM 13 } 14 Disconnect-VIServer -server $esxi -Confirm:$false -ErrorAction SilentlyContinue 15} 16 17sleep 600 18 19Connect-VIServer -server $vcenterip -user $vcenteruser -password $vcenterpw 20Get-VApp -Name \u0026#39;Nested-VCF-*\u0026#39; | Remove-VApp -DeletePermanently -Confirm:$false 21Disconnect-VIServer * -Confirm:$false Conclusion By tweaking the script, I managed to work around the shared datastore requirement and adapt the deployment to my limited homelab resources. In an upcoming post, I'll dive deeper into the technical details of these modifications and share the full deployment process.\nFeel free to have a look at the script and try it in your homelab :)\nhttps://github.com/p3t35/vcf-automated-lab-deployment\nStay tuned! üöÄ\n","link":"http://localhost:1313/post/automated-vcf-deployment/","section":"post","tags":["homelab","intelnuc","vcf","vsphere","esxi","powershell","powercli"],"title":"Automated VCF Deployment"},{"body":"","link":"http://localhost:1313/tags/powercli/","section":"tags","tags":null,"title":"Powercli"},{"body":"","link":"http://localhost:1313/tags/powershell/","section":"tags","tags":null,"title":"Powershell"},{"body":"Opensense To maintain a clear separation between my homelab and home network, I needed a firewall. A physical firewall wasn‚Äôt an option since I wanted to keep the lab compact. This left me with the choice of a virtual firewall. After researching, I narrowed down the options to three main solutions: pfSense, OPNsense, and VyOS.\nPreviously, I had experience using pfSense for virtual firewalling. For my homelab, I decided to try OPNsense to gain familiarity with it. While VyOS is also on my radar for the future, my immediate focus was on getting OPNsense up and running.\nThis article focuses on configuring OPNsense for a homelab setup. I won‚Äôt delve into the detailed installation process, as many other blogs already cover that topic.\nHere‚Äôs an overview of my network configuration:\nThe OPNsense virtual machine (VM) has an \u0026quot;external\u0026quot; IP address connected to the home network. On the LAN side, it uses multiple VLAN interfaces. This setup enables the creation of distinct networks, allowing me to simulate a \u0026quot;real-world\u0026quot; environment. It‚Äôs particularly useful for mirroring customer environments with various VLANs.\nAll external traffic is NATed through the external IP address to access the internet and other resources. For management and access, I rely on a Linux jump host with xrdp installed. On the OPNsense side, I configured DNAT rules to forward traffic for RDP (port 3389) and SSH (port 22) from the external IP to the internal IP of the jump host. This approach ensures I can access the lab from any device. To configure the \u0026quot;LAN\u0026quot; interface on OPNsense, I created a port group that trunks all VLANs, as shown in the screenshot below: ESXi Networking Next, let‚Äôs look at the networking setup for the two ESXi hosts in the lab.\nBoth hosts utilize vSwitch0, which connects the ESXi hosts and the OPNsense VM to the home network. For internal homelab traffic, I created a Distributed Switch. This switch includes the trunk port group for OPNsense and VLAN-tagged port groups for services like vCenter, NSX Manager, and other VMs.\nUsing a Distributed Switch is essential for enabling MAC learning, a feature not available on standard vSwitches. MAC learning is particularly useful when running nested ESXi hosts. For more details on this topic, check out the blog post by my colleague Daniel Krieger, which highlights the advantages of MAC learning over promiscuous mode:\nMAC Learning is your friend\nThe internal Distributed Switch uses a direct connection between the two ESXi hosts, providing a 2.5 Gbps connection for all internal traffic, VM traffic, and vMotion. Additionally, the ESXi hosts have a vmkernel interface (vmk1) on this Distributed Switch.\nESXi USB NIC Driver If you‚Äôve examined the setup closely, you might have noticed a USB NIC attached to one of the ESXi hosts. This is because one NUC has 2 x 2.5 Gbps ports, while the other has 1 x 2.5 Gbps port plus an attached 1 Gbps USB NIC.\nTo enable the USB NIC, I installed the USB Network Native Driver for ESXi, a community-provided fling.\nYou can find more details about the USB Network Native Driver for ESXi here: https://community.broadcom.com/flings/home\n","link":"http://localhost:1313/post/network-setup/","section":"post","tags":["homelab","opensense","networking","vsphere"],"title":"Network Setup"},{"body":"","link":"http://localhost:1313/tags/networking/","section":"tags","tags":null,"title":"Networking"},{"body":"","link":"http://localhost:1313/tags/opensense/","section":"tags","tags":null,"title":"Opensense"},{"body":"My Current Lab Setup When it comes to building my home lab, my main goal is to keep things simple, low-cost, and low-overhead. It‚Äôs a place where I can experiment with new technologies, test out virtual environments, and try out different configurations‚Äîwithout draining my wallet or disrupting the rest of the household. Here‚Äôs a breakdown of my current setup and how I‚Äôve optimized it for efficiency and independence.\nThe primary goal of this setup is to create an isolated environment where everything can be tested without risk to the outside network. To achieve this, all traffic within the homelab is routed through a virtual instance of OpnSense.\nAccess to the lab is strictly controlled; I connect exclusively via RDP to a Linux jumphost. This ensures that the homelab remains fully isolated from external systems and networks.\nIn a future post, I will dive deeper into the configuration of OpnSense and the internal network setup.\nThe Hardware My lab is built around two Intel NUCs, each equipped with 64GB of RAM. These compact machines provide enough horsepower to run a variety of workloads, but they‚Äôre small enough to keep everything running quietly and with minimal energy consumption.\nBut the best part? The lab is completely independent from our home network, so if something breaks or if I‚Äôm running some resource-heavy workloads, Netflix and the rest of the household‚Äôs devices still work without interruption. It's the perfect balance between experimenting with new tech and maintaining a smooth home environment.\nRoom for Growth While my lab setup is functional for now, there‚Äôs always room for improvement. There‚Äôs still plenty of capacity for growth, and new hardware is always welcome‚Äîwhether it's adding more storage, integrating additional virtual machines, or even expanding to more powerful components like additional NUCs or networking gear. But like with any hobby or project, everything in due time. I‚Äôm not in a rush to make the lab bigger or more complex; I‚Äôm more focused on optimizing what I have and gradually expanding as needed.\nWhy This Setup Works for Me This lab setup strikes the perfect balance between cost and functionality. The Intel NUCs are compact and efficient, and they‚Äôre flexible enough to run a variety of virtualized environments without creating too much overhead. Plus, the fact that the lab is completely isolated from the rest of the home network means that I can take risks, experiment, and tinker without worrying about affecting our daily activities.\nIn the future, I‚Äôm excited to keep building this lab. Whether it‚Äôs adding automation, testing cloud solutions, there‚Äôs always room to improve and learn. For now, though, I‚Äôm happy with this setup and looking forward to where it can take me next.\nI created a page where I track the current status of my homelab with network diagrams, pictures and BOMs.\nSetup\n","link":"http://localhost:1313/post/home-lab-setup/","section":"post","tags":["homelab"],"title":"Home Lab Setup"},{"body":"Hardware 11.03.2025 - Little Upgrade\nThanks to my colleague Timur for the 3D printed surprise :) Network BOM (Bill of Material) Component Specifications Intel NUC13 i5 2 x 32GB RAM, 1TB SATA SSD, 1TB NVMe SSD Intel NUC13 i7 2 x 32GB RAM, 1TB SATA SSD, 1TB NVMe SSD HORACO 2,5 GbE Managed Switch 8 Port 8 Port, 2,5 Gbit Ethernet ","link":"http://localhost:1313/setup/","section":"","tags":["homelab","intelnuc"],"title":"Setup"},{"body":"Hey there! I‚Äôm Peter Summa, a Cloud Engineer at evoila, a tech-driven company based in Germany.\nWith a strong focus on virtualization and cloud technologies, I specialize in solutions like vSphere, NSX, Avi and Tanzu. Over the years, I‚Äôve worked hands-on with everything from traditional VMware environments to modern cloud-native technologies like Terraform, Kubernetes, and VMware Cloud Foundation (VCF).\nThis blog is a place where I share my experiences, insights, and the lessons learned while building homelab setups, optimizing cloud environments, and deploying complex virtualized infrastructures. Whether you're looking to automate your workflows with Terraform, explore Kubernetes in a hybrid cloud setup, or dive into the intricacies of VCF, there‚Äôs something here for you.\nWhen I‚Äôm not working on client projects or tinkering with my own homelab, I‚Äôm researching the latest trends in cloud engineering or finding ways to simplify complex technical problems.\nThanks for stopping by! Feel free to reach out if you want to chat about virtualization, cloud engineering, or any of the technologies I‚Äôm passionate about. Let's connect and collaborate!\nCredly\nImpressum Die folgenden Angaben basieren auf den Vorgaben in ¬ß 5 DDG:\nPeter Summa\nLindenstr. 2\n74238 Krautheim\nKontakt mail: petersumma96@gmail.com\nHaftung f√ºr und √úberpr√ºfung von Inhalten: Durch die Vorgaben in ¬ß 5 DDG bin ich als Webmaster f√ºr die Inhalte meines Blogs verantwortlich. Gleichzeitig befreien mich ¬ß¬ß 8 bis einschlie√ülich ¬ß 10 TMG von der Verantwortung, √ºbermittelte oder gespeicherte fremde Inhalte zu √ºberwachen. Trotzdem bin ich mir meiner Pflicht bewusst, der Sperrung und Entfernung von Informationen nachzukommen, wie es geltende Gesetze vorgeben.\n","link":"http://localhost:1313/about/","section":"","tags":null,"title":"About me"}]